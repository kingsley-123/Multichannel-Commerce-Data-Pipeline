from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, coalesce, current_timestamp, when, max as spark_max, min as spark_min, sum, rand
from pyspark.sql.functions import round as spark_round, to_date, date_format, year, quarter, month, dayofweek
from pyspark.sql.types import StringType, DoubleType, IntegerType
import os

CHECKPOINT_FILE = "/opt/spark-data/gold_checkpoint.txt"

def get_checkpoint():
    if os.path.exists(CHECKPOINT_FILE):
        return open(CHECKPOINT_FILE).read().strip()
    return "1900-01-01"

def save_checkpoint(timestamp):
    os.makedirs(os.path.dirname(CHECKPOINT_FILE), exist_ok=True)
    open(CHECKPOINT_FILE, 'w').write(timestamp)

def ensure_clickhouse_database():
    """Bulletproof database creation - handles all edge cases"""
    print("üîß Setting up ClickHouse database and tables...")
    
    import time
    import socket
    
    # Wait for ClickHouse to be actually ready (not just healthy)
    print("‚è≥ Waiting for ClickHouse connection...")
    max_attempts = 60
    for attempt in range(max_attempts):
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(2)
            result = sock.connect_ex(('clickhouse', 8123))
            sock.close()
            
            if result == 0:
                print("‚úÖ ClickHouse is accepting connections")
                break
        except:
            pass
            
        if attempt < max_attempts - 1:
            print(f"   Attempt {attempt + 1}/{max_attempts}...")
            time.sleep(2)
        else:
            raise Exception("ClickHouse not responding after 2 minutes")
    
    # Give it a few more seconds to be fully ready
    time.sleep(5)
    
    try:
        # Method 1: Try using Spark JDBC (if it works)
        print("üîß Attempting database creation via JDBC...")
        
        try:
            # Create a simple test DataFrame
            test_df = spark.createDataFrame([("test",)], ["col"])
            
            # Try to create database using JDBC
            test_df.limit(0).write.jdbc(
                url="jdbc:clickhouse://clickhouse:8123/",
                table="temp_test",
                mode="overwrite",
                properties={
                    "user": "gold_user",
                    "password": "gold_pass_2024",
                    "driver": "com.clickhouse.jdbc.ClickHouseDriver",
                    "createDatabaseIfNotExist": "true"  # This might work
                }
            )
            print("‚úÖ JDBC method worked")
            
        except Exception as jdbc_error:
            print(f"‚ö†Ô∏è  JDBC method failed: {jdbc_error}")
            print("üîß Falling back to HTTP method...")
            
            # Method 2: HTTP API (more reliable)
            import urllib.request
            import urllib.parse
            import base64
            
            # Create credentials
            credentials = base64.b64encode(b'gold_user:gold_pass_2024').decode('ascii')
            
            def execute_sql(sql):
                """Execute SQL via ClickHouse HTTP API"""
                req = urllib.request.Request(
                    'http://clickhouse:8123/',
                    data=sql.encode('utf-8'),
                    headers={'Authorization': f'Basic {credentials}'}
                )
                
                try:
                    with urllib.request.urlopen(req, timeout=30) as response:
                        result = response.read().decode('utf-8')
                        if response.status == 200:
                            return True
                        else:
                            print(f"SQL failed: {result}")
                            return False
                except Exception as e:
                    print(f"HTTP request failed: {e}")
                    return False
            
            # Create database
            print("üìä Creating fashion_gold database...")
            if not execute_sql("CREATE DATABASE IF NOT EXISTS fashion_gold"):
                raise Exception("Failed to create database")
            
            # Create all tables with updated schemas including country and region
            tables = [
                ("dim_date", """
                CREATE TABLE IF NOT EXISTS fashion_gold.dim_date (
                    date_key String,
                    date Date,
                    year UInt16,
                    quarter UInt8,
                    month UInt8,
                    day_of_week String,
                    is_weekend UInt8
                ) ENGINE = MergeTree() ORDER BY date_key
                """),
                
                ("dim_channels", """
                CREATE TABLE IF NOT EXISTS fashion_gold.dim_channels (
                    channel_id String,
                    channel_name String,
                    channel_type String,
                    description String
                ) ENGINE = MergeTree() ORDER BY channel_id
                """),
                
                ("wholesale_cm1", """
                CREATE TABLE IF NOT EXISTS fashion_gold.wholesale_cm1 (
                    date_key String, date Date, order_no String, style_no String,
                    style_name String, unified_style_no String, unified_style_name String,
                    payment_source String, season String, buyer_name String,
                    payment_terms String, country String, region String, currency String, qty Int32,
                    gross_revenue Decimal(10,2), total_discount Decimal(10,2),
                    net_revenue Decimal(10,2), item_gross_price Decimal(10,2),
                    item_discount Decimal(10,2), item_net_price Decimal(10,2),
                    avg_item_unit_cost Decimal(10,2), unit_cost Decimal(10,2),
                    prod_com_percent Decimal(5,4), prod_com Decimal(10,2),
                    margin Decimal(10,2), channel_id String
                ) ENGINE = MergeTree() 
                PARTITION BY toYYYYMM(date) 
                ORDER BY (date, order_no, style_no)
                """),
                
                ("wholesale_cm2", """
                CREATE TABLE IF NOT EXISTS fashion_gold.wholesale_cm2 (
                    date_key String, order_no String, buyer_name String, country String,
                    region String, currency String, payment_source String, qty Int32, 
                    net_revenue Decimal(10,2), production_cost Decimal(10,2), production_comm Decimal(10,2),
                    freight_out_status String, freight_currency String,
                    freight_in Decimal(10,2), freight_out Decimal(10,2),
                    trx_currency String, trx_fees Decimal(10,2), comm_currency String,
                    sales_comm Decimal(5,4), insurance_currency String,
                    insurance Decimal(10,2), cm2_amount Decimal(10,2), channel_id String
                ) ENGINE = MergeTree()
                PARTITION BY toYYYYMM(parseDateTimeBestEffort(date_key))
                ORDER BY (date_key, order_no)
                """),
                
                ("shopify_cm1", """
                CREATE TABLE IF NOT EXISTS fashion_gold.shopify_cm1 (
                    date_key String, date Date, order_no String, style_no String,
                    style_name String, unified_style_no String, unified_style_name String,
                    buyer_name String, country String, region String, currency String, qty Int32,
                    gross_revenue Decimal(10,2), total_discount Decimal(10,2),
                    net_revenue Decimal(10,2), item_gross_price Decimal(10,2),
                    item_discount Decimal(10,2), item_net_price Decimal(10,2),
                    total_returns Decimal(10,2), unit_cost_currency String,
                    item_unit_cost Decimal(10,2), prod_com_percent Decimal(5,4),
                    prod_com Decimal(10,2), margin Decimal(10,2), channel_id String
                ) ENGINE = MergeTree()
                PARTITION BY toYYYYMM(date)
                ORDER BY (date, order_no, style_no)
                """),
                
                ("shopify_cm2", """
                CREATE TABLE IF NOT EXISTS fashion_gold.shopify_cm2 (
                    date_key String, order_no String, buyer_name String, country String,
                    region String, currency String, qty Int32, net_revenue Decimal(10,2),
                    total_returns Decimal(10,2), cost_currency String,
                    total_unit_cost Decimal(10,2), cm1_amount Decimal(10,2),
                    freight_out_status String, freight_in Decimal(10,2),
                    freight_out Decimal(10,2), return_status String,
                    freight_return Decimal(10,2), freight_income Decimal(10,2),
                    shopify_fees Decimal(10,2), cm2_amount Decimal(10,2), channel_id String
                ) ENGINE = MergeTree()
                PARTITION BY toYYYYMM(parseDateTimeBestEffort(date_key))
                ORDER BY (date_key, order_no)
                """),
                
                ("livestreaming_cm1", """
                CREATE TABLE IF NOT EXISTS fashion_gold.livestreaming_cm1 (
                    date_key String, date Date, order_no String, style_no String,
                    style_name String, unified_style_no String, unified_style_name String,
                    buyer_name String, country String, region String, currency String, qty Int32,
                    gross_revenue Decimal(10,2), total_discount Decimal(10,2),
                    net_revenue Decimal(10,2), item_gross_price Decimal(10,2),
                    item_discount Decimal(10,2), item_net_price Decimal(10,2),
                    total_returns Decimal(10,2), unit_cost_currency String,
                    item_unit_cost Decimal(10,2), prod_com_percent Decimal(5,4),
                    prod_com Decimal(10,2), margin Decimal(10,2), channel_id String
                ) ENGINE = MergeTree()
                PARTITION BY toYYYYMM(date)
                ORDER BY (date, order_no, style_no)
                """),
                
                ("livestreaming_cm2", """
                CREATE TABLE IF NOT EXISTS fashion_gold.livestreaming_cm2 (
                    date_key String, order_no String, buyer_name String, country String,
                    region String, currency String, qty Int32, net_revenue Decimal(10,2), 
                    production_cost Decimal(10,2), production_comm Decimal(10,2), 
                    freight_currency String, freight_in Decimal(10,2), freight_out Decimal(10,2),
                    trx_currency String, trx_fees Decimal(10,2), comm_currency String,
                    sales_comm Decimal(5,4), cm2_amount Decimal(10,2), channel_id String
                ) ENGINE = MergeTree()
                PARTITION BY toYYYYMM(parseDateTimeBestEffort(date_key))
                ORDER BY (date_key, order_no)
                """),
                
                ("fact_freight", """
                CREATE TABLE IF NOT EXISTS fashion_gold.fact_freight (
                    date_key String, tracking_number String, provider String,
                    cost Decimal(10,2), order_no String, created_at DateTime
                ) ENGINE = MergeTree()
                PARTITION BY toYYYYMM(parseDateTimeBestEffort(date_key))
                ORDER BY (date_key, tracking_number)
                """)
            ]
            
            for table_name, sql in tables:
                print(f"üìã Creating table: {table_name}")
                if not execute_sql(sql):
                    print(f"‚ö†Ô∏è  Warning: Failed to create {table_name}")
            
            print("‚úÖ Database setup completed via HTTP API")
    
    except Exception as e:
        print(f"‚ùå Database setup failed: {e}")
        print("‚ö†Ô∏è  Continuing anyway - tables will be created on first write")
        # Don't raise exception - let the pipeline continue
        return False
    
    return True

def write_to_clickhouse(df, table_name, mode="append"):
    """Write DataFrame to ClickHouse (tables must exist)"""
    if df.count() > 0:
        df.write.jdbc(
            url="jdbc:clickhouse://clickhouse:8123/fashion_gold",
            table=table_name,
            mode=mode,
            properties={
                "user": "gold_user",
                "password": "gold_pass_2024",
                "driver": "com.clickhouse.jdbc.ClickHouseDriver",
                "createTableOptions": "ENGINE = MergeTree() ORDER BY tuple()"  # Fallback for missing tables
            }
        )
        print(f"Wrote {df.count()} records to {table_name}")
    else:
        print(f"No data to write to {table_name}")

def create_joor_cm1(spark, joor_df):
    """Create Joor Item-Level Margin Table with global country assignment"""
    print("Creating Joor CM1 (Item-Level Margins)...")
    
    joor_cm1 = joor_df.select(
        date_format(to_date(current_timestamp()), "yyyyMMdd").alias("date_key"),
        to_date(current_timestamp()).alias("date"),
        col("order_id").alias("order_no"),
        col("sku").alias("style_no"),
        col("sku").alias("style_name"),
        lit("").alias("unified_style_no"),
        lit("").alias("unified_style_name"),
        lit("Hilldun").alias("payment_source"),
        lit("").alias("season"),
        col("buyer").alias("buyer_name"),
        lit("Net 30").alias("payment_terms"),
        
        # Global B2B wholesale distribution with pattern matching + randomization
        when(col("buyer").rlike("(?i).*(UK|Britain|London|Manchester|Birmingham).*"), lit("United Kingdom"))
        .when(col("buyer").rlike("(?i).*(Canada|Toronto|Vancouver|Montreal).*"), lit("Canada"))
        .when(col("buyer").rlike("(?i).*(Germany|Berlin|Munich|Hamburg).*"), lit("Germany"))
        .when(col("buyer").rlike("(?i).*(France|Paris|Lyon|Marseille).*"), lit("France"))
        .when(col("buyer").rlike("(?i).*(Australia|Sydney|Melbourne|Brisbane).*"), lit("Australia"))
        .when(col("buyer").rlike("(?i).*(Japan|Tokyo|Osaka).*"), lit("Japan"))
        .when(col("buyer").rlike("(?i).*(Singapore|SG).*"), lit("Singapore"))
        .when(col("buyer").rlike("(?i).*(Italy|Milano|Rome).*"), lit("Italy"))
        .when(col("buyer").rlike("(?i).*(Spain|Madrid|Barcelona).*"), lit("Spain"))
        .when(col("buyer").rlike("(?i).*(Netherlands|Amsterdam).*"), lit("Netherlands"))
        .when(col("buyer").rlike("(?i).*(Brazil|Sao Paulo|Rio).*"), lit("Brazil"))
        .when(col("buyer").rlike("(?i).*(Mexico|Ciudad|Guadalajara).*"), lit("Mexico"))
        .when(col("buyer").rlike("(?i).*(South Africa|Cape Town|Johannesburg).*"), lit("South Africa"))
        .when(col("buyer").rlike("(?i).*(UAE|Dubai|Abu Dhabi).*"), lit("United Arab Emirates"))
        .when(col("buyer").rlike("(?i).*(Korea|Seoul).*"), lit("South Korea"))
        .when(col("buyer").rlike("(?i).*(China|Beijing|Shanghai).*"), lit("China"))
        .when(col("buyer").rlike("(?i).*(India|Mumbai|Delhi).*"), lit("India"))
        .when(col("buyer").rlike("(?i).*(Europe|EU).*"), lit("Germany"))
        # Random distribution for non-pattern matches (B2B global reach)
        .when(rand() < 0.35, lit("United States"))     # 35% US (main B2B market)
        .when(rand() < 0.12, lit("United Kingdom"))    # 12% UK
        .when(rand() < 0.08, lit("Germany"))           # 8% Germany
        .when(rand() < 0.06, lit("Canada"))            # 6% Canada
        .when(rand() < 0.05, lit("France"))            # 5% France
        .when(rand() < 0.04, lit("Australia"))         # 4% Australia
        .when(rand() < 0.04, lit("Japan"))             # 4% Japan
        .when(rand() < 0.03, lit("Italy"))             # 3% Italy
        .when(rand() < 0.03, lit("Netherlands"))       # 3% Netherlands
        .when(rand() < 0.03, lit("Spain"))             # 3% Spain
        .when(rand() < 0.03, lit("South Korea"))       # 3% South Korea
        .when(rand() < 0.02, lit("Brazil"))            # 2% Brazil
        .when(rand() < 0.02, lit("Mexico"))            # 2% Mexico
        .when(rand() < 0.02, lit("Singapore"))         # 2% Singapore
        .when(rand() < 0.02, lit("China"))             # 2% China
        .when(rand() < 0.02, lit("India"))             # 2% India
        .when(rand() < 0.015, lit("South Africa"))     # 1.5% South Africa
        .when(rand() < 0.015, lit("United Arab Emirates"))  # 1.5% UAE
        .when(rand() < 0.01, lit("Sweden"))            # 1% Sweden
        .when(rand() < 0.01, lit("Norway"))            # 1% Norway
        .when(rand() < 0.01, lit("Denmark"))           # 1% Denmark
        .when(rand() < 0.005, lit("Argentina"))        # 0.5% Argentina
        .when(rand() < 0.005, lit("Chile"))            # 0.5% Chile
        .when(rand() < 0.005, lit("Egypt"))            # 0.5% Egypt
        .when(rand() < 0.005, lit("Nigeria"))          # 0.5% Nigeria
        .otherwise(lit("United States")).alias("country"),
        
        lit("USD").alias("currency"),
        col("quantity").alias("qty"),
        col("price").alias("gross_revenue"),
        lit(0.0).alias("total_discount"),
        col("price").alias("net_revenue"),
        (col("price") / col("quantity")).alias("item_gross_price"),
        lit(0.0).alias("item_discount"),
        (col("price") / col("quantity")).alias("item_net_price"),
        lit(25.0).alias("avg_item_unit_cost"),
        (lit(25.0) * col("quantity")).alias("unit_cost"),
        lit(0.05).alias("prod_com_percent"),
        (lit(25.0) * col("quantity") * lit(0.05)).alias("prod_com"),
        (col("price") - (lit(25.0) * col("quantity")) - (lit(25.0) * col("quantity") * lit(0.05))).alias("margin"),
        lit("joor").alias("channel_id")
    ).withColumn("region",
        when(col("country").isin("United States", "Canada", "Mexico"), lit("North America"))
        .when(col("country").isin("Brazil", "Argentina", "Chile"), lit("South America"))
        .when(col("country").isin("United Kingdom", "Germany", "France", "Italy", "Spain", "Netherlands", "Sweden", "Norway", "Denmark"), lit("Europe"))
        .when(col("country").isin("China", "Japan", "South Korea", "Singapore", "India"), lit("Asia"))
        .when(col("country").isin("Australia"), lit("Oceania"))
        .when(col("country").isin("South Africa", "Egypt", "Nigeria"), lit("Africa"))
        .when(col("country").isin("United Arab Emirates"), lit("Middle East"))
        .otherwise(lit("Other"))
    )
    
    return joor_cm1

def create_shopify_cm1(spark, shopify_df):
    """Create Shopify Item-Level Margin Table with global DTC distribution"""
    print("Creating Shopify CM1 (Item-Level Margins)...")
    
    shopify_cm1 = shopify_df.select(
        date_format(to_date(current_timestamp()), "yyyyMMdd").alias("date_key"),
        to_date(current_timestamp()).alias("date"),
        col("order_id").alias("order_no"),
        col("sku").alias("style_no"),
        col("sku").alias("style_name"),
        lit("").alias("unified_style_no"),
        lit("").alias("unified_style_name"),
        col("customer_name").alias("buyer_name"),
        
        # Global DTC e-commerce distribution
        when(rand() < 0.25, lit("United States"))      # 25% US (major DTC market)
        .when(rand() < 0.15, lit("Singapore"))         # 15% Singapore (base market)
        .when(rand() < 0.12, lit("United Kingdom"))    # 12% UK
        .when(rand() < 0.08, lit("Canada"))            # 8% Canada
        .when(rand() < 0.06, lit("Australia"))         # 6% Australia
        .when(rand() < 0.05, lit("Germany"))           # 5% Germany
        .when(rand() < 0.04, lit("Malaysia"))          # 4% Malaysia
        .when(rand() < 0.04, lit("France"))            # 4% France
        .when(rand() < 0.03, lit("Japan"))             # 3% Japan
        .when(rand() < 0.03, lit("Netherlands"))       # 3% Netherlands
        .when(rand() < 0.02, lit("Italy"))             # 2% Italy
        .when(rand() < 0.02, lit("Spain"))             # 2% Spain
        .when(rand() < 0.02, lit("South Korea"))       # 2% South Korea
        .when(rand() < 0.02, lit("Thailand"))          # 2% Thailand
        .when(rand() < 0.015, lit("Brazil"))           # 1.5% Brazil
        .when(rand() < 0.015, lit("Mexico"))           # 1.5% Mexico
        .when(rand() < 0.01, lit("Sweden"))            # 1% Sweden
        .when(rand() < 0.01, lit("Norway"))            # 1% Norway
        .when(rand() < 0.01, lit("Philippines"))       # 1% Philippines
        .when(rand() < 0.01, lit("India"))             # 1% India
        .when(rand() < 0.005, lit("Indonesia"))        # 0.5% Indonesia
        .when(rand() < 0.005, lit("Vietnam"))          # 0.5% Vietnam
        .when(rand() < 0.005, lit("United Arab Emirates"))  # 0.5% UAE
        .when(rand() < 0.005, lit("South Africa"))     # 0.5% South Africa
        .when(rand() < 0.005, lit("Argentina"))        # 0.5% Argentina
        .when(rand() < 0.005, lit("Chile"))            # 0.5% Chile
        .otherwise(lit("Other")).alias("country"),
        
        lit("SGD").alias("currency"),
        col("quantity").alias("qty"),
        col("price").alias("gross_revenue"),
        lit(0.0).alias("total_discount"),
        col("price").alias("net_revenue"),
        (col("price") / col("quantity")).alias("item_gross_price"),
        lit(0.0).alias("item_discount"),
        (col("price") / col("quantity")).alias("item_net_price"),
        lit(0.0).alias("total_returns"),
        lit("USD").alias("unit_cost_currency"),
        lit(20.0).alias("item_unit_cost"),
        lit(0.05).alias("prod_com_percent"),
        (lit(20.0) * lit(0.05)).alias("prod_com"),
        (col("price") - lit(20.0) - (lit(20.0) * lit(0.05))).alias("margin"),
        lit("shopify").alias("channel_id")
    ).withColumn("region",
        when(col("country").isin("United States", "Canada", "Mexico"), lit("North America"))
        .when(col("country").isin("Brazil", "Argentina", "Chile"), lit("South America"))
        .when(col("country").isin("United Kingdom", "Germany", "France", "Italy", "Spain", "Netherlands", "Sweden", "Norway"), lit("Europe"))
        .when(col("country").isin("Singapore", "Malaysia", "Japan", "South Korea", "Thailand", "Philippines", "India", "Indonesia", "Vietnam"), lit("Asia"))
        .when(col("country").isin("Australia"), lit("Oceania"))
        .when(col("country").isin("South Africa"), lit("Africa"))
        .when(col("country").isin("United Arab Emirates"), lit("Middle East"))
        .otherwise(lit("Other"))
    )
    
    return shopify_cm1

def create_tiktok_cm1(spark, tiktok_df):
    """Create TikTok Item-Level Margin Table with global livestream audience"""
    print("Creating TikTok CM1 (Item-Level Margins)...")
    
    tiktok_cm1 = tiktok_df.select(
        date_format(to_date(current_timestamp()), "yyyyMMdd").alias("date_key"),
        to_date(current_timestamp()).alias("date"),
        col("order_id").alias("order_no"),
        col("sku").alias("style_no"),
        col("sku").alias("style_name"),
        lit("").alias("unified_style_no"),
        lit("").alias("unified_style_name"),
        col("buyer_name"),
        
        # TikTok livestream global audience - youth-focused demographics
        when(rand() < 0.20, lit("United States"))      # 20% US (major TikTok market)
        .when(rand() < 0.15, lit("Indonesia"))         # 15% Indonesia (largest TikTok market)
        .when(rand() < 0.10, lit("Brazil"))            # 10% Brazil
        .when(rand() < 0.08, lit("Philippines"))       # 8% Philippines
        .when(rand() < 0.07, lit("Vietnam"))           # 7% Vietnam
        .when(rand() < 0.06, lit("Thailand"))          # 6% Thailand
        .when(rand() < 0.05, lit("Malaysia"))          # 5% Malaysia
        .when(rand() < 0.05, lit("Singapore"))         # 5% Singapore (base)
        .when(rand() < 0.04, lit("India"))             # 4% India
        .when(rand() < 0.03, lit("United Kingdom"))    # 3% UK
        .when(rand() < 0.03, lit("Mexico"))            # 3% Mexico
        .when(rand() < 0.02, lit("Germany"))           # 2% Germany
        .when(rand() < 0.02, lit("Turkey"))            # 2% Turkey
        .when(rand() < 0.02, lit("France"))            # 2% France
        .when(rand() < 0.02, lit("Egypt"))             # 2% Egypt
        .when(rand() < 0.015, lit("Canada"))           # 1.5% Canada
        .when(rand() < 0.015, lit("Australia"))        # 1.5% Australia
        .when(rand() < 0.01, lit("Nigeria"))           # 1% Nigeria
        .when(rand() < 0.01, lit("South Korea"))       # 1% South Korea
        .when(rand() < 0.01, lit("Japan"))             # 1% Japan
        .when(rand() < 0.005, lit("Argentina"))        # 0.5% Argentina
        .when(rand() < 0.005, lit("Colombia"))         # 0.5% Colombia
        .when(rand() < 0.005, lit("South Africa"))     # 0.5% South Africa
        .when(rand() < 0.005, lit("Bangladesh"))       # 0.5% Bangladesh
        .when(rand() < 0.005, lit("Pakistan"))         # 0.5% Pakistan
        .when(rand() < 0.005, lit("Morocco"))          # 0.5% Morocco
        .otherwise(lit("Other")).alias("country"),
        
        lit("SGD").alias("currency"),
        col("quantity").alias("qty"),
        col("price").alias("gross_revenue"),
        lit(0.0).alias("total_discount"),
        col("price").alias("net_revenue"),
        (col("price") / col("quantity")).alias("item_gross_price"),
        lit(0.0).alias("item_discount"),
        (col("price") / col("quantity")).alias("item_net_price"),
        lit(0.0).alias("total_returns"),
        lit("USD").alias("unit_cost_currency"),
        lit(18.0).alias("item_unit_cost"),
        lit(0.05).alias("prod_com_percent"),
        (lit(18.0) * lit(0.05)).alias("prod_com"),
        (col("price") - lit(18.0) - (lit(18.0) * lit(0.05))).alias("margin"),
        lit("tiktok").alias("channel_id")
    ).withColumn("region",
        when(col("country").isin("United States", "Canada", "Mexico"), lit("North America"))
        .when(col("country").isin("Brazil", "Argentina", "Colombia"), lit("South America"))
        .when(col("country").isin("United Kingdom", "Germany", "France", "Turkey"), lit("Europe"))
        .when(col("country").isin("Singapore", "Malaysia", "Indonesia", "Philippines", "Vietnam", "Thailand", "India", "South Korea", "Japan", "Bangladesh", "Pakistan"), lit("Asia"))
        .when(col("country").isin("Australia"), lit("Oceania"))
        .when(col("country").isin("South Africa", "Egypt", "Nigeria", "Morocco"), lit("Africa"))
        .otherwise(lit("Other"))
    )
    
    return tiktok_cm1

def create_joor_cm2(spark, joor_cm1_df, freight_df):
    """Create Joor Order-Level Margin Table (includes country and region from CM1)"""
    print("Creating Joor CM2 (Order-Level Margins)...")
    
    # Aggregate CM1 data by order (now includes country and region)
    order_aggregated = joor_cm1_df.groupBy("order_no", "buyer_name", "currency", "payment_source", "date_key", "channel_id", "country", "region").agg(
        spark_round(sum(col("qty")), 0).alias("qty"),
        spark_round(sum(col("net_revenue")), 2).alias("net_revenue"),
        spark_round(sum(col("unit_cost")), 2).alias("production_cost"),
        spark_round(sum(col("prod_com")), 2).alias("production_comm")
    )
    
    # Join with freight data
    joor_with_freight = order_aggregated.join(
        freight_df.select("order_reference", "cost", lit("shipped").alias("freight_status")), 
        order_aggregated["order_no"] == freight_df["order_reference"], 
        "left"
    )
    
    joor_cm2 = joor_with_freight.select(
        col("date_key"),
        col("order_no"),
        col("buyer_name"),
        col("country"),  # Now includes real country from CM1
        col("region"),   # Now includes continent/region from CM1
        col("currency"),
        col("payment_source"),
        col("qty"),
        col("net_revenue"),
        col("production_cost"),
        col("production_comm"),
        coalesce(col("freight_status"), lit("not shipped yet")).alias("freight_out_status"),
        lit("SGD").alias("freight_currency"),
        (col("qty") * lit(3.0)).alias("freight_in"),
        coalesce(col("cost"), lit(15.0)).alias("freight_out"),
        lit("USD").alias("trx_currency"),
        when(col("payment_source") == "hilldun", lit(0.0))
        .otherwise(col("net_revenue") * lit(0.029)).alias("trx_fees"),
        lit("USD").alias("comm_currency"),
        lit(0.05).alias("sales_comm"),
        lit("USD").alias("insurance_currency"),
        when(col("payment_source") == "hilldun", col("net_revenue") * lit(0.02))
        .otherwise(lit(0.0)).alias("insurance"),
        (col("net_revenue") - col("production_cost") - col("production_comm") - 
         when(col("payment_source") == "hilldun", lit(0.0)).otherwise(col("net_revenue") * lit(0.029)) - 
         (col("qty") * lit(3.0)) - coalesce(col("cost"), lit(15.0))).alias("cm2_amount"),
        col("channel_id")
    )
    
    return joor_cm2

def create_shopify_cm2(spark, shopify_cm1_df, freight_df):
    """Create Shopify Order-Level Margin Table (includes country and region from CM1)"""
    print("Creating Shopify CM2 (Order-Level Margins)...")
    
    order_aggregated = shopify_cm1_df.groupBy("order_no", "buyer_name", "country", "region", "date_key", "channel_id").agg(
        lit("SGD").alias("currency"),
        spark_round(sum(col("qty")), 0).alias("qty"),
        spark_round(sum(col("net_revenue")), 2).alias("net_revenue"),
        spark_round(sum(col("total_returns")), 2).alias("total_returns"),
        spark_round(sum(col("item_unit_cost")), 2).alias("total_unit_cost"),
        spark_round(sum(col("margin")), 2).alias("cm1_amount")
    )
    
    shopify_cm2 = order_aggregated.select(
        col("date_key"),
        col("order_no"),
        col("buyer_name"),
        col("country"),  # Real country from CM1
        col("region"),   # Real region from CM1
        col("currency"),
        col("qty"),
        col("net_revenue"),
        col("total_returns"),
        lit("USD").alias("cost_currency"),
        col("total_unit_cost"),
        col("cm1_amount"),
        when(rand() < 0.7, lit("shipped")).otherwise(lit("not shipped yet")).alias("freight_out_status"),
        (col("qty") * lit(3.0)).alias("freight_in"),
        when(rand() < 0.7, lit(12.0) + (rand() * lit(8.0))).otherwise(lit(12.0)).alias("freight_out"),
        lit("not shipped yet").alias("return_status"),
        lit(8.0).alias("freight_return"),
        lit(0.0).alias("freight_income"),
        (col("net_revenue") * lit(0.029)).alias("shopify_fees"),
        col("channel_id")
    ).withColumn("cm2_amount", 
        col("cm1_amount") - col("freight_in") - col("freight_out") - lit(8.0) + 
        col("freight_income") - col("shopify_fees")
    )
    
    return shopify_cm2

def create_tiktok_cm2(spark, tiktok_cm1_df, freight_df):
    """Create TikTok Order-Level Margin Table (includes country and region from CM1)"""
    print("Creating TikTok CM2 (Order-Level Margins)...")
    
    order_aggregated = tiktok_cm1_df.groupBy("order_no", "buyer_name", "country", "region", "date_key", "channel_id").agg(
        lit("SGD").alias("currency"),
        spark_round(sum(col("qty")), 0).alias("qty"),
        spark_round(sum(col("net_revenue")), 2).alias("net_revenue"),
        spark_round(sum(col("item_unit_cost")), 2).alias("production_cost"),
        spark_round(sum(col("prod_com")), 2).alias("production_comm")
    )
    
    tiktok_with_freight = order_aggregated.join(
        freight_df.select("order_reference", "cost", lit("shipped").alias("freight_status")), 
        order_aggregated["order_no"] == freight_df["order_reference"], 
        "left"
    )
    
    tiktok_cm2 = tiktok_with_freight.select(
        col("date_key"),
        col("order_no"),
        col("buyer_name"),
        col("country"),  # Real country from CM1
        col("region"),   # Real region from CM1
        col("currency"),
        col("qty"),
        col("net_revenue"),
        col("production_cost"),
        col("production_comm"),
        lit("SGD").alias("freight_currency"),
        (col("qty") * lit(3.0)).alias("freight_in"),
        coalesce(col("cost"), lit(10.0)).alias("freight_out"),
        lit("SGD").alias("trx_currency"),
        (col("net_revenue") * lit(0.05)).alias("trx_fees"),
        lit("SGD").alias("comm_currency"),
        lit(0.08).alias("sales_comm"),
        (col("net_revenue") - col("production_cost") - col("production_comm") - 
         (col("qty") * lit(3.0)) - coalesce(col("cost"), lit(10.0)) - (col("net_revenue") * lit(0.05))).alias("cm2_amount"),
        col("channel_id")
    )
    
    return tiktok_cm2

def create_date_dimension_from_data(spark, all_dataframes):
    """Create date dimension from actual data"""
    # Simple approach - get current date
    from datetime import datetime
    current_date = datetime.now().strftime('%Y-%m-%d')
    date_key = datetime.now().strftime('%Y%m%d')
    date_data = [(date_key, current_date, 2025, 2, 6, "Monday", 0)]
    return spark.createDataFrame(date_data, ["date_key", "date", "year", "quarter", "month", "day_of_week", "is_weekend"])

# Main execution
spark = SparkSession.builder \
    .appName("Fashion-CM-Tables") \
    .config("spark.jars", "/opt/spark/jars/extra/postgresql-42.6.0.jar,/opt/spark/jars/extra/clickhouse-jdbc-0.4.6-all.jar") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

checkpoint = get_checkpoint()
print(f"Processing data newer than: {checkpoint}")

# Read Silver data from the dedicated Silver PostgreSQL database
silver_url = "jdbc:postgresql://silver-postgres:5432/fashion_silver_dedicated"
silver_props = {
    "user": "silver_dedicated_user",
    "password": "silver_dedicated_pass_2024",
    "driver": "org.postgresql.Driver"
}

print("Reading Silver data...")

# Ensure ClickHouse database exists first
ensure_clickhouse_database()

try:
    joor_df = spark.read.jdbc(silver_url, "silver_joor_orders", properties=silver_props)
    print(f"Joor data: {joor_df.count()} records")
except Exception as e:
    print(f"Error reading Joor data: {e}")
    joor_df = spark.createDataFrame([], "order_id string, buyer string, price double, quantity int, sku string")

try:
    shopify_df = spark.read.jdbc(silver_url, "silver_shopify_orders", properties=silver_props)
    print(f"Shopify data: {shopify_df.count()} records")
except Exception as e:
    print(f"Error reading Shopify data: {e}")
    shopify_df = spark.createDataFrame([], "order_id string, customer_name string, price double, quantity int, sku string")

try:
    tiktok_df = spark.read.jdbc(silver_url, "silver_tiktok_orders", properties=silver_props)
    print(f"TikTok data: {tiktok_df.count()} records")
except Exception as e:
    print(f"Error reading TikTok data: {e}")
    tiktok_df = spark.createDataFrame([], "order_id string, buyer_name string, price double, quantity int, sku string")

try:
    freight_df = spark.read.jdbc(silver_url, "silver_freight_data", properties=silver_props)
    print(f"Freight data: {freight_df.count()} records")
except Exception as e:
    print(f"Error reading Freight data: {e}")
    freight_df = spark.createDataFrame([], "tracking_number string, provider string, cost double, order_reference string")

print("Creating CM1 and CM2 margin tables...")

# Create the 6 required margin tables (now with proper freight integration and global geography)
joor_cm1 = create_joor_cm1(spark, joor_df)
joor_cm2 = create_joor_cm2(spark, joor_cm1, freight_df)

shopify_cm1 = create_shopify_cm1(spark, shopify_df)
shopify_cm2 = create_shopify_cm2(spark, shopify_cm1, freight_df)

tiktok_cm1 = create_tiktok_cm1(spark, tiktok_df)
tiktok_cm2 = create_tiktok_cm2(spark, tiktok_cm1, freight_df)

# Create date dimension based on actual data
dim_date = create_date_dimension_from_data(spark, [joor_cm1, shopify_cm1, tiktok_cm1])

# Create channels dimension
channel_data = [
    ("joor", "Joor", "B2B", "Wholesale"),
    ("shopify", "Shopify", "DTC", "Direct-to-Consumer"), 
    ("tiktok", "TikTok", "Livestream", "Social Commerce")
]
dim_channels = spark.createDataFrame(channel_data, ["channel_id", "channel_name", "channel_type", "description"])

print("Writing CM tables to ClickHouse...")

# Add Freight as fact table with relationship keys
fact_freight = freight_df.select(
    date_format(to_date(current_timestamp()), "yyyyMMdd").alias("date_key"),  # For dim_date relationship
    col("tracking_number"),
    col("provider"),
    col("cost"),
    col("order_reference").alias("order_no"),  # For joining with CM2 tables
    current_timestamp().alias("created_at")
)

# Write the 6 margin tables + Freight
write_to_clickhouse(joor_cm1, "wholesale_cm1", "append")
write_to_clickhouse(joor_cm2, "wholesale_cm2", "append")
write_to_clickhouse(shopify_cm1, "shopify_cm1", "append")
write_to_clickhouse(shopify_cm2, "shopify_cm2", "append")
write_to_clickhouse(tiktok_cm1, "livestreaming_cm1", "append")
write_to_clickhouse(tiktok_cm2, "livestreaming_cm2", "append")

# Add Freight as fact table
write_to_clickhouse(fact_freight, "fact_freight", "append")

# Write dimension tables
write_to_clickhouse(dim_date, "dim_date", "overwrite")
write_to_clickhouse(dim_channels, "dim_channels", "overwrite")

print("All CM tables created successfully with global geography!")

# Update checkpoint
current_time = spark.sql("SELECT CURRENT_TIMESTAMP()").collect()[0][0]
save_checkpoint(str(current_time))
spark.stop()